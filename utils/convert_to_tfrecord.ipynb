{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc9436be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import inflection as inf\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8a048b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "############################# CHANGE FILENAMES HERE ###################################\n",
    "#######################################################################################\n",
    "\n",
    "annotation_dir = \"../annotations/splits\"\n",
    "determiners = [\"a\", \"an\", \"all\", \"any\", \"every\", \"my\", \"your\", \"this\", \"that\", \"these\", \"those\", \"some\", \"many\", \"few\", \"both\", \"neither\", \"little\", \"much\", \"either\", \"our\", \"no\", \"several\", \"half\", \"each\", \"the\"]\n",
    "# filenames = [\"annotations_val.json\", \"annotations_train.json\", \"annotations_test.json\"]\n",
    "filenames = [\"train_001.json\", \"train_005.json\", \"train_010.json\", \"train_025.json\", \"train_050.json\"]\n",
    "save_dir = \"../annotations/tfrecords/splits/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3825c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = json.load(open(\"../annotations/annotations_val.json\", 'r'))\n",
    "categories = temp[\"categories\"]\n",
    "n_categories = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c76d7f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(value=[tf.io.encode_png(value).numpy()])\n",
    "    )\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode()]))\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def int64_feature_list(value):\n",
    "    \"\"\"Returns a list of int_list from a int.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def float_feature_list(value):\n",
    "    \"\"\"Returns a list of float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def object_features_list(objects): \n",
    "    feature_list = []\n",
    "    for obj in objects:\n",
    "        feature_list.append(int64_feature(obj[\"id\"]))\n",
    "    return tf.train.FeatureList(feature=feature_list)\n",
    "    \n",
    "\n",
    "def create_example(image_id, sample):\n",
    "    caption = sample[\"image\"][\"caption\"]\n",
    "    file_name = sample[\"image\"][\"file_name\"]\n",
    "    det = caption.split()[0]\n",
    "    noun = \" \".join(caption.split()[1:])\n",
    "    determiners.index(det)\n",
    "    noun_id = 0 \n",
    "    \n",
    "    noun = inf.singularize(noun)\n",
    "    for cat in categories: \n",
    "        if cat[\"name\"] == noun: \n",
    "            noun_id = cat[\"id\"]\n",
    "\n",
    "    det_one_hot = [0 for i in range(len(determiners))]\n",
    "    det_one_hot[determiners.index(det)] = 1\n",
    "    noun_one_hot = [0 for i in range(len(categories))]\n",
    "    noun_one_hot[noun_id] = 1\n",
    "    caption_one_hot = det_one_hot + noun_one_hot\n",
    "    \n",
    "    max_bboxes = 20 \n",
    "    \n",
    "    input_one_hot = []\n",
    "    output_one_hot =[]\n",
    "\n",
    "    inputs = sample[\"inputs\"]\n",
    "    outputs = sample[\"outputs\"]\n",
    "    \n",
    "    for ann in outputs: \n",
    "        one_hot = [0 for i in range(n_categories)]\n",
    "        one_hot[ann[\"category_id\"]] = 1\n",
    "        output_one_hot.append(ann[\"bbox\"] + [1] + one_hot)\n",
    "\n",
    "    for j in range(len(output_one_hot), max_bboxes): \n",
    "        one_hot = [0 for i in range(n_categories)]\n",
    "        output_one_hot.append([0,0,0,0] + [0] + one_hot)\n",
    "            \n",
    "    for ann in inputs: \n",
    "        one_hot = [0 for i in range(n_categories)]\n",
    "        one_hot[ann[\"category_id\"]] = 1\n",
    "        input_one_hot.append(ann[\"bbox\"] + [1] + one_hot + [ann[\"liqLevel\"]])\n",
    "        \n",
    "    for j in range(len(input_one_hot), max_bboxes): \n",
    "        one_hot = [0 for i in range(n_categories)]\n",
    "        input_one_hot.append([0,0,0,0] + [0] + one_hot)\n",
    "        \n",
    "    context = {\n",
    "        # \"image\": image_feature(image),\n",
    "        \"file_name\": bytes_feature(file_name),\n",
    "        \"image_id\": int64_feature(image_id),\n",
    "        \"caption\": bytes_feature(caption),\n",
    "        \"caption_one_hot\": int64_feature_list(caption_one_hot),\n",
    "        \"areas\": int64_feature_list([ann['area'] for ann in inputs]), \n",
    "        \"category_ids\": int64_feature_list([ann['category_id'] for ann in inputs]), \n",
    "        \"output_category_ids\": int64_feature_list([ann['category_id'] for ann in outputs]),        \n",
    "        \"output_areas\": int64_feature_list([ann['area'] for ann in outputs]),\n",
    "    }\n",
    "\n",
    "    feature_list = {\n",
    "        \"input_bboxes\": tf.train.FeatureList(feature=[tf.train.Feature(int64_list=tf.train.Int64List(value=ann['bbox'])) for ann in inputs]),\n",
    "        \"output_bboxes\": tf.train.FeatureList(feature=[tf.train.Feature(int64_list=tf.train.Int64List(value=ann['bbox'])) for ann in outputs]), \n",
    "        \"input_one_hot\": tf.train.FeatureList(feature=[tf.train.Feature(float_list=tf.train.FloatList(value=val)) for val in input_one_hot]),\n",
    "        \"output_one_hot\": tf.train.FeatureList(feature=[tf.train.Feature(int64_list=tf.train.Int64List(value=val)) for val in output_one_hot]),\n",
    "    }\n",
    "     \n",
    "    return tf.train.SequenceExample(context=tf.train.Features(feature=context), feature_lists=tf.train.FeatureLists(feature_list=feature_list))\n",
    "\n",
    "def parse_tfrecord_fn(example, labeled=True):\n",
    "    feature_description = {\n",
    "        \"file_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "        #         \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"caption\": tf.io.VarLenFeature(tf.string),\n",
    "        \"caption_one_hot\": tf.io.VarLenFeature(tf.int64),\n",
    "        \"areas\": tf.io.VarLenFeature(tf.int64),\n",
    "        \"category_ids\": tf.io.VarLenFeature(tf.int64),\n",
    "        \"output_category_ids\": tf.io.VarLenFeature(tf.int64),\n",
    "        \"output_areas\": tf.io.VarLenFeature(tf.int64)\n",
    "    }\n",
    "\n",
    "    sequence_features = {\n",
    "        \"input_bboxes\": tf.io.VarLenFeature(tf.int64),\n",
    "        \"output_bboxes\": tf.io.VarLenFeature(tf.int64),\n",
    "        \"input_one_hot\": tf.io.VarLenFeature(tf.float32),\n",
    "        \"output_one_hot\": tf.io.VarLenFeature(tf.int64)\n",
    "    }\n",
    "    context, sequence = tf.io.parse_single_sequence_example(example, context_features=feature_description,\n",
    "                                                            sequence_features=sequence_features)\n",
    "\n",
    "    example = {**context, **sequence}\n",
    "    for key in example.keys():\n",
    "        if type(example[key]) == tf.sparse.SparseTensor:\n",
    "            if (example[key].dtype == \"string\"):\n",
    "                example[key] = tf.sparse.to_dense(example[key], default_value='b')\n",
    "            else:\n",
    "                example[key] = tf.sparse.to_dense(example[key])\n",
    "\n",
    "    prefix = \"../DetermiNetProject/Assets/StreamingAssets/dataset/\"\n",
    "    raw = tf.io.read_file(prefix + example[\"file_name\"])\n",
    "    example[\"image\"] = tf.io.decode_png(raw, channels=3)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0218532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../annotations/tfrecords/splits/001\n",
      "../annotations/tfrecords/splits/005\n",
      "../annotations/tfrecords/splits/010\n",
      "../annotations/tfrecords/splits/025\n",
      "../annotations/tfrecords/splits/050\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for filename in filenames:\n",
    "    annotation_filepath = os.path.join(annotation_dir, filename)\n",
    "    dataset = json.load(open(annotation_filepath, 'r'))\n",
    "    images = dataset[\"images\"]\n",
    "    input_annotations = dataset[\"input_oracle_annotations\"]\n",
    "    output_annotations = dataset[\"annotations\"]\n",
    "    n_samples = 4096\n",
    "    n_tfrecords = len(images) // n_samples\n",
    "    if len(images) % n_samples: \n",
    "        n_tfrecords += 1 \n",
    "    \n",
    "    split_dir = os.path.join(save_dir, filename.split(\".\")[0].split(\"_\")[1])\n",
    "    print(split_dir)\n",
    "    if not os.path.exists(split_dir): \n",
    "        os.makedirs(split_dir)\n",
    "\n",
    "    dataset_samples = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for ann in input_annotations: \n",
    "        dataset_samples[ann[\"image_id\"]][\"inputs\"].append(ann)\n",
    "        \n",
    "    for ann in output_annotations: \n",
    "        dataset_samples[ann[\"image_id\"]][\"outputs\"].append(ann)\n",
    "\n",
    "    for image in images: \n",
    "        dataset_samples[image[\"id\"]][\"image\"] = image\n",
    "\n",
    "    keys = list(dataset_samples.keys())\n",
    "    for tfrec_num in range(n_tfrecords): \n",
    "        sample_keys = keys[tfrec_num*n_samples : (tfrec_num + 1) * n_samples]\n",
    "        \n",
    "        with tf.io.TFRecordWriter(\n",
    "            split_dir + \"/file_%.2i-%i.tfrec\" % (tfrec_num, len(sample_keys))\n",
    "        ) as writer:\n",
    "            for key in sample_keys:\n",
    "                dataset_sample = dataset_samples[key] \n",
    "                input_objects = dataset_samples[key][\"input\"]\n",
    "                output_objects = dataset_samples[key][\"output\"]\n",
    "                example = create_example(key, dataset_sample)\n",
    "                writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c78511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c4d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e066273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "98f310addaf7dac00cd5965e6c1c6cb4dc304674e0e6e4d0010a991e9aec678e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
